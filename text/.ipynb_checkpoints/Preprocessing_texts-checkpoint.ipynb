{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1e992e",
   "metadata": {},
   "source": [
    "## What is NLP (Natural Language Processing)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39d1f13",
   "metadata": {},
   "source": [
    "Natural language processing (NLP) refers to the branch of computer science—and more specifically, the branch of artificial intelligence concerned with giving computers the ability to understand text and spoken words in much the same way human beings can.\n",
    "\n",
    "### Some NLP tasks\n",
    "\n",
    "* automatic translation\n",
    "* speech recognition\n",
    "* question answering\n",
    "* text summarization\n",
    "* sentiment analysis\n",
    "* topic modelling\n",
    "* Named Entity Recognition (NER)\n",
    "* speech to text and text to speech\n",
    "* text and question generatiion\n",
    "* error correction\n",
    "* Word Sense Disambiguation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7accf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"data/nlp.PNG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339b45e",
   "metadata": {},
   "source": [
    "### What is text pre-processing and why do we need it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b71503a",
   "metadata": {},
   "source": [
    "Most of the time, the texts that we need to analyze, or - the data we have, is not in an ideal state. It is often the case that when doing NLP related tasks, we tend to underestimate the time and effort needed to get the data (the texts) into a state in which they are ready for further analysis or prediction.\n",
    "\n",
    "Text pre-processing is often the first and <b> a very important </b>, (although perhaps not very interesting) step of any NLP task. \n",
    "\n",
    "For computers, understanding human language is not the easiest task. Computers can easily understands <b> structured </b> data, such as spreadsheets, or tables in a database.\n",
    "\n",
    "However, language - whether in a written form (such as tweets, articles, reviews, novels, poems, plays, etc.) or in a spoken form is compeletly <b>unstructured </b>. Pre- procssing transforms text into a more digestible form for the computer, so that our NLP methods (based on machine-learning algorithms, e.g.) can preform better.\n",
    "\n",
    "It is sometimes a real effort to discover in which ways our data is messy, and to think about how we can clean it.\n",
    "\n",
    "However, it is really important to put in that work, because if our data isn't <i> clean </i>, the results of our analysis won't be <i>clean</i> either.\n",
    "\n",
    "The pre-processing steps for a problem depend mainly on the domain and the problem itself, hence, we don’t need to apply all steps to every problem. \n",
    "\n",
    "In this notebook, some of the common pre-processing steps will be named. Depending on your data and your task (as well as the NLP method you plan to use) you will need only some or every step in this guide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31555df1",
   "metadata": {},
   "source": [
    "### Pre-processing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7835b127",
   "metadata": {},
   "source": [
    "* lower casing\n",
    "* tokenization\n",
    "* removing punctuation\n",
    "* removing URLs\n",
    "* removing stop words\n",
    "* stemming\n",
    "* lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a2f799",
   "metadata": {},
   "source": [
    "### Importing libraries (that might be familiar) we need for this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2fb31b",
   "metadata": {},
   "source": [
    "We can also do this stuff by ourselves, and not use libraries. Usually, the idea is not to reinvent the wheel, but if you see that something works better when you do it - then do it yourself :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48dfe08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a2269",
   "metadata": {},
   "source": [
    "### Reading in our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e7441",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cactus.txt') as fh:\n",
    "    text = fh.read()\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c0b69a",
   "metadata": {},
   "source": [
    "### Removing newline characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237fd4f",
   "metadata": {},
   "source": [
    "We can do this many ways, but one is using the ```replace()``` string method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b39b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace(\"\\n\", \"\")\n",
    "text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e0b319",
   "metadata": {},
   "source": [
    "### Changing to lower/upper case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c490b8",
   "metadata": {},
   "source": [
    "Usually, we would need to change to lowercase. This is because 'Happy' and 'happy' are the same word, but since they are spelled differently, the computer would count it as two. We lowercase the text to reduce the size of the vocabulary of our text data.\n",
    "\n",
    "```lower()``` and  ```upper()``` are string methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae9074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddba311",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_text = text_lowercase(text)\n",
    "lower_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910bd943",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a2b36",
   "metadata": {},
   "source": [
    "We can also do this with the string ```split()``` method as well as e.g. ```sent_tokenize()``` from the package nltk.\n",
    "There's many ways to do it - but the main idea is - if we want to make a word-based, or sentence-based analysis (e.g. this makes sense for sentiment analysis), we need to split our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ecc409",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = lower_text.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4bd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fcef8d",
   "metadata": {},
   "source": [
    "We see here that theres trailing whitespaces in our list items.\n",
    "Let's remove them by remembering list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dd4658",
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped = [s.strip() for s in sentences]\n",
    "stripped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f05c15",
   "metadata": {},
   "source": [
    "deleting the empty list item:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c776fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list = [x for x in stripped if x != '']\n",
    "new_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afa8032",
   "metadata": {},
   "source": [
    "As we said in the previous notebook, we can also used the ```re``` module for tokenization (splitting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca585e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = re.split(r'[.?!]\\s*', lower_text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed316d4",
   "metadata": {},
   "source": [
    "Or, NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689825fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6835936c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(lower_text)\n",
    "sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a60cce",
   "metadata": {},
   "source": [
    "If we wanted to have the words as list elements (word tokenization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb1f742",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize (lower_text)\n",
    "words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3dbf7",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf00a8f1",
   "metadata": {},
   "source": [
    "Something from the previous notebook (converting multiple whitespaces and tabs to one, and then splitting and replacing the punctuation using the re module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/cactus.txt') as fh:\n",
    "    text = fh.read()\n",
    "text = re.sub('\\s+', ' ', text)    \n",
    "sentences = re.split(r'[\\.\\?\\!]\\s*', text)\n",
    "print(sentences[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9b6511",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "words = [w for w in words if w not in string.punctuation]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7bcffe",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033565da",
   "metadata": {},
   "source": [
    "In the file `data/stopwords.txt` we have a list of stopwords for the English language. Let's try to clean our tokens from these words. For this we have to read the stopword list first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd5de2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/stopwords.txt') as fp:\n",
    "    stopwords = [word.rstrip() for word in fp.readlines()]\n",
    "stopwords    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdfd265",
   "metadata": {},
   "source": [
    "Now to remove the stopwords from the list `tokens` we can use a List Comprension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b85056",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(words)) \n",
    "tokens = [token for token in words if token not in stopwords]\n",
    "len(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4365f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd45dad",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4dc01e",
   "metadata": {},
   "source": [
    "Stemming is the process of getting the root form of a word. Stem or root is the part to which inflectional affixes (-ed, -ize, -de, -s, etc.) are added. The stem of a word is created by removing the prefix or suffix of a word. So, stemming a word may not result in actual words.\n",
    "\n",
    "<u>Example</u>:\n",
    "\n",
    "```\n",
    "books      --->    book\n",
    "looked     --->    look\n",
    "denied     --->    deni\n",
    "flies      --->    fli\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1924e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae02fb4a",
   "metadata": {},
   "source": [
    "Porter stemmer algorithm is a pretty standard (from 1979) solution for stemming (https://tartarus.org/martin/PorterStemmer/).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea5bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stems = [stemmer.stem(word) for word in tokens]\n",
    "stems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3644f48",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0256fa",
   "metadata": {},
   "source": [
    "Like stemming, lemmatization also converts a word to its root form. However,l emmatization is a more powerful operation, and it takes into consideration morphological analysis of the words. It returns the lemma which is the base form of all its inflectional forms. Lemmatization ensures that the root word belongs to the language. We will get valid words if we use lemmatization. \n",
    "\n",
    "\n",
    "\n",
    "E.g.\n",
    "\n",
    "```\n",
    "books      --->    book\n",
    "looked     --->    look\n",
    "denied     --->    deny\n",
    "flies      --->    fly\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1588873",
   "metadata": {},
   "source": [
    "In NLTK, we use the WordNetLemmatizer to get the lemmas of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39337455",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c696c6a0",
   "metadata": {},
   "source": [
    "In order to achieve lemmatization, we also need Part-Of-Speech tagging. \n",
    "What is this and why do we need it?\n",
    "POS-tagging refers to categorizing words in a text (corpus) in correspondence with a particular part of speech, depending on the definition of the word and its context (verbs, nouns, etc.). This allows us to preform lemmatization, since the lemma depends on the part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a7de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper function to map NTLK position tags\n",
    "# Full list is available here: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb336045",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_sentence = []\n",
    "# Get tags\n",
    "word_pos_tags = nltk.pos_tag(tokens)\n",
    "word_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410cdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the position tag and lemmatize the word/token\n",
    "for idx, tag in enumerate(word_pos_tags):\n",
    "    lemmatized_sentence.append(lemmatizer.lemmatize(tag[0], get_wordnet_pos(tag[1])))\n",
    "# the lemmatize() method takes the word and the pos tag as parameters\n",
    "lemmatized_text = \" \".join(lemmatized_sentence)\n",
    "lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a3b127",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise Preprocessing</b>\n",
    "<p>Pre-process the file 'data/cat.txt' - the output should be the clean, lemmatized text.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aff088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
