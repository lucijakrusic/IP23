{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def0a9c3",
   "metadata": {},
   "source": [
    "## Introduction to Natural Language Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "969194be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabbe2ff",
   "metadata": {},
   "source": [
    "## Topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5aa27c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../img/TM.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"../img/TM.png\")\n",
    "# source: https://www.analyticsvidhya.com/blog/2021/07/topic-modelling-with-lda-a-hands-on-introduction/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f31c2d",
   "metadata": {},
   "source": [
    "Topic modeling is a type of statistical modeling for discovering abstract topics that occur in a collection of documents. At its core, topic modeling is about uncovering hidden structure in text data, and it is a powerful tool for organizing and understanding large collections of unstructured data.\n",
    "\n",
    "Topic models provide a simple way to analyze large volumes of unlabeled text. A \"topic\" consists of a cluster of words that frequently occur together. Using contextual clues, topic models can connect words with similar meanings and distinguish between uses of words with multiple meanings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358afe23",
   "metadata": {},
   "source": [
    "### Topic modelling in the context of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0418c38",
   "metadata": {},
   "source": [
    "Topic modeling is an <b>unsupervised</b> machine learning task. The aim of topic modeling is to identify the main topics that occur in a collection of documents. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics.\n",
    "\n",
    "The reason it's an unsupervised task is because we don't know in advance what the topics are or how many topics there are. There's no \"correct\" answer that we're trying to predict. Instead, the algorithm tries to find patterns in the data and uses these patterns to determine the topics.\n",
    "\n",
    "There are several algorithms for topic modeling, including Latent Dirichlet Allocation (LDA), Non-negative Matrix Factorization (NMF), and others. Each of these algorithms has its own assumptions and methods for determining the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6480ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../img/tm2.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"../img/tm2.png\")\n",
    "# source: https://www.cognub.com/index.php/cognitive-platform/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c0dbcb",
   "metadata": {},
   "source": [
    "### The practical part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ec459e",
   "metadata": {},
   "source": [
    "Before we start with any Machine Learning or Natural Language Processing, we need data. Here, we are using the BBC News dataset. It contains articles from BBC News.\n",
    "\n",
    "In Python, we can use the pandas library to load the data from a csv file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f558bda6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737d22ab",
   "metadata": {},
   "source": [
    "#### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "305bba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/bbc-text.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "888ad87c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2225, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e579d810",
   "metadata": {},
   "source": [
    "#### Check out the first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d9b0ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>howard hits back at mongrel jibe michael howar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>blair prepares to name poll date tony blair is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>henman hopes ended in dubai third seed tim hen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wilkinson fit to face edinburgh england captai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>last star wars  not for children  the sixth an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  tv future in the hands of viewers with home th...\n",
       "1  worldcom boss  left books alone  former worldc...\n",
       "2  tigers wary of farrell  gamble  leicester say ...\n",
       "3  yeading face newcastle in fa cup premiership s...\n",
       "4  ocean s twelve raids box office ocean s twelve...\n",
       "5  howard hits back at mongrel jibe michael howar...\n",
       "6  blair prepares to name poll date tony blair is...\n",
       "7  henman hopes ended in dubai third seed tim hen...\n",
       "8  wilkinson fit to face edinburgh england captai...\n",
       "9  last star wars  not for children  the sixth an..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a263f",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431a45e",
   "metadata": {},
   "source": [
    "LDA is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. In the context of topic modeling, the \"observations\" are words in documents and the \"unobserved groups\" are topics. The \"why some parts of the data are similar\" is because similar documents share topics.\n",
    "\n",
    "Basically, itâ€™s a way of explaining why some documents are similar to others (they are about the same topics). It uses a probabilistic graphical model where each document is assumed to be a mixture of various topics, and each word is probabilistically drawn from one topic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1746ef24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../img/lda.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url= \"../img/lda.png\")\n",
    "# Blei, D.M. (2012) Probabilistic Topic Models. Communications of the ACM, 55, 77-84. http://dx.doi.org/10.1145/2133806.2133826\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c8aa7d",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f08828",
   "metadata": {},
   "source": [
    "The first step in any NLP task is text preprocessing. This usually involves converting all the text to lowercase, removing punctuation and stop words, and tokenization (breaking the text down into individual words).\n",
    "\n",
    "We're also going to do some extra steps specific to topic modelling: lemmatization, which reduces words to their root form. For instance, \"running\" would be reduced to \"run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ed41d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c0eb2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Text Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lower case\n",
    "    text = text.lower()\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    # Tokenization\n",
    "    text = text.split()\n",
    "    # Remove stop words and Lemmatize\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in stopwords.words('english')]\n",
    "    # Join words to a single string\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e78762",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['text'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34b3e91",
   "metadata": {},
   "source": [
    "#### Digression - Lambda function:\n",
    " \n",
    " - An anonymous function in Python is one that has no name when it is defined. In Python, the lambda keyword is used to define anonymous functions rather than the def keyword, which is used for normal functions. As a result, lambda functions are another name for anonymous functions.\n",
    "\n",
    " -  here, x is the argument (an individual value from the 'text' column), and preprocess_text(x) is the expression. This lambda function applies the preprocess_text function to each value in the 'text' column.\n",
    "\n",
    "- so, the entire line of code is taking each value in the 'text' column of the dataframe, applying the preprocess_text function to it, and then storing the result in a new column called 'processed_text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae8d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de32e0",
   "metadata": {},
   "source": [
    "Now that we have our text preprocessed, let's start with our first topic modelling algorithm: Latent Dirichlet Allocation (LDA). LDA assumes that every document is a mixture of topics and that every word in the document is attributable to the document's topics.\n",
    "\n",
    "We'll use the LDA implementation from the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500d5397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6fe00",
   "metadata": {},
   "source": [
    "### Create a CountVectorizer for parsing/counting words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f15f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "term_matrix = count_vectorizer.fit_transform(df['processed_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec53697",
   "metadata": {},
   "source": [
    "### Digression - CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37417b5b",
   "metadata": {},
   "source": [
    "- CountVectorizer is a class provided by the sklearn.feature_extraction.text module in the scikit-learn library. It's used to convert a collection of text documents to a matrix of token (word) counts.\n",
    "\n",
    "<b>When using CountVectorizer, the process usually involves the following steps:</b>\n",
    "\n",
    "1. The text is tokenized, meaning it's split into individual words according to some rule. By default, this is done by splitting the text on whitespace and punctuation.\n",
    "\n",
    "2. The words are counted. For each document, CountVectorizer maintains a count of how many times each word appeared.\n",
    "\n",
    "3. A document-term matrix is created. Each row of the matrix represents a document, and each column represents a unique word from across all documents. The entry in the ith row and the jth column of the matrix is the count of word j in document i.\n",
    "\n",
    "4. The output is a sparse matrix representation of the documents, which can be used as input to a machine learning model.\n",
    "\n",
    "#### max_df: \n",
    "- used for removing terms that appear too frequently, also known as \"corpus-specific stop words\"\n",
    "- For example, max_df=0.95 means \"ignore terms that appear in more than 95% of the documents\"\n",
    "- These common words usually don't carry important meaning and are often removed.\n",
    "\n",
    "\n",
    "#### min_df:\n",
    "- used for removing terms that appear too infrequently\n",
    "- For example, min_df=2 means \"ignore terms that appear in less than 2 documents\"\n",
    "- The intuition behind this approach is that words that appear only once or a few times in the entire corpus might be typos, rare words or otherwise irrelevant to the analysis, thus can be safely ignored.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e776f6c4",
   "metadata": {},
   "source": [
    "### Create and fit the LDA model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecda6abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(term_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef41f02",
   "metadata": {},
   "source": [
    "In this section, the n_components stands for the number of topics we want to retreive. Playing with this parameter may also influence the quality of our results, since we are talking about clustering. Usually the best thing is to play around until we get a good evaluation score - or we simply figure that out topics look nice enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64f6013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the topics from the model\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print (\"Topic \", idx, \" \".join(count_vectorizer.get_feature_names()[i] for i in topic.argsort()[:-10 - 1:-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a921ab0c",
   "metadata": {},
   "source": [
    " - the argsort() function from numpy gets the indices that would sort the topic array. It returns the indices that would sort an array.\n",
    "\n",
    "-  [:-11:-1] slice is getting the last 10 values from the sorted indices in reverse order. This gives you the indices of the 10 highest values in the topic array, which are the top 10 words for that topic.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e5796",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317eb7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00c3c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the feature names from count vectorizer\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "\n",
    "# Get the topics and their top 10 words for LDA\n",
    "lda_topics = [[(feature_names[i], topic[i]) for i in topic.argsort()[:-11:-1]] for topic in lda.components_]\n",
    "\n",
    "for i, topic in enumerate(lda_topics):\n",
    "    wc = WordCloud(background_color=\"white\", max_words=2000)\n",
    "    wc.generate_from_frequencies(dict(topic))\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f'Topic {i+1}')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88435afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea93c0",
   "metadata": {},
   "source": [
    "### How do we interpret these topics? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9726373",
   "metadata": {},
   "source": [
    "1. Check the most significant words: Start by looking at the words with the highest weights in each topic. These are the words that are most representative of the topic according to the model.\n",
    "\n",
    "2. Understand the common theme: Try to find a common theme or category among these words. For instance, if the top words are \"doctor\", \"patient\", \"hospital\", and \"medicine\", then a good interpretation of the topic might be \"Healthcare\" or \"Medicine\".\n",
    "\n",
    "3. Use your domain knowledge: Your own understanding of the subject matter can be very useful in interpreting the topics. For instance, if you're analyzing news articles and one of the topics contains words like \"election\", \"votes\", \"candidate\", and \"campaign\", then you could interpret this as a \"Politics\" topic.\n",
    "\n",
    "4. Check the related documents: Another way to interpret the topics is to look at some documents that are heavily associated with each topic. By reading these documents, you might get a better understanding of what the topic represents.\n",
    "\n",
    "5. Keep in mind that topics are probabilistic: Topic modelling algorithms like LDA are probabilistic, which means that they provide a probability distribution over all words for each topic, and a probability distribution over all topics for each document. The topics and the document-topic associations are not definitive but rather represent the algorithm's best guess based on the data and its own internal mathematics.\n",
    "\n",
    "6. Don't overinterpret: Finally, remember that not all topics might make perfect sense, and that's okay. Topic models are statistical models that try to find structure in the data, but sometimes this structure doesn't map perfectly onto human interpretability.\n",
    "\n",
    "Interpreting topics from topic modelling is more of an art than a science, requiring a mix of understanding the model's output, using your own domain knowledge, and making sensible judgments.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c922ef",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise 1 - LDA</b>\n",
    "<p>\n",
    "<li>Think of the topic names for the identified words in the word clouds. What would be the common themes?</li>\n",
    "\n",
    "</p>\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa37a25",
   "metadata": {},
   "source": [
    "### Literature and references"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b26b6",
   "metadata": {},
   "source": [
    "- \"Latent Dirichlet Allocation\" by David M. Blei, Andrew Y. Ng, Michael I. Jordan (https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) - This is the original paper that introduced LDA.\n",
    "\n",
    "- Topic modeling in Python with NLTK and Gensim (https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24) - This blog post provides a practical guide to topic modeling in Python.\n",
    "\n",
    "- Nonnegative Matrix Factorization (NMF) (https://medium.com/python-in-plain-english/topic-modelling-with-nmf-in-python-194eb6ae04a5) - Practical Guide - This post explains how to perform topic modeling using NMF with practical examples. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
